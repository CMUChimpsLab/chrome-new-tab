{
  "_args": [
    [
      "scrape-it@5.1.4",
      "/Users/yiyingding/Desktop/Safesea/chrome-new-tab/remote-website"
    ]
  ],
  "_from": "scrape-it@5.1.4",
  "_id": "scrape-it@5.1.4",
  "_inBundle": false,
  "_integrity": "sha512-vLXSl8xjMs/DZ55IA8Mazk8qY/AYVYhbLIz8WIfRrw7d9hL+dOt+Av/4QRVyL3CVWDb8Bhqx0Cj/64/FanPMJg==",
  "_location": "/scrape-it",
  "_phantomChildren": {
    "css-select": "1.2.0",
    "dom-serializer": "0.1.0",
    "entities": "1.1.1",
    "htmlparser2": "3.9.2",
    "lodash.assignin": "4.2.0",
    "lodash.bind": "4.2.1",
    "lodash.defaults": "4.2.0",
    "lodash.filter": "4.6.0",
    "lodash.flatten": "4.4.0",
    "lodash.foreach": "4.5.0",
    "lodash.map": "4.6.0",
    "lodash.merge": "4.6.2",
    "lodash.pick": "4.4.0",
    "lodash.reduce": "4.6.0",
    "lodash.reject": "4.6.0",
    "lodash.some": "4.6.0"
  },
  "_requested": {
    "type": "version",
    "registry": true,
    "raw": "scrape-it@5.1.4",
    "name": "scrape-it",
    "escapedName": "scrape-it",
    "rawSpec": "5.1.4",
    "saveSpec": null,
    "fetchSpec": "5.1.4"
  },
  "_requiredBy": [
    "/"
  ],
  "_resolved": "https://registry.npmjs.org/scrape-it/-/scrape-it-5.1.4.tgz",
  "_spec": "5.1.4",
  "_where": "/Users/yiyingding/Desktop/Safesea/chrome-new-tab/remote-website",
  "author": {
    "name": "Ionică Bizău",
    "email": "bizauionica@gmail.com",
    "url": "https://ionicabizau.net"
  },
  "blah": {
    "h_img": "https://i.imgur.com/j3Z0rbN.png",
    "cli": "scrape-it-cli",
    "installation": [
      {
        "h2": "FAQ"
      },
      {
        "p": "Here are some frequent questions and their answers."
      },
      {
        "h3": "1. How to parse scrape pages?"
      },
      {
        "p": "`scrape-it` has only a simple request module for making requests. That means you cannot directly parse ajax pages with it, but in general you will have those scenarios:"
      },
      {
        "ol": [
          "**The ajax response is in JSON format.** In this case, you can make the request directly, without needing a scraping library.",
          "**The ajax response gives you HTML back.** Instead of calling the main website (e.g. example.com), pass to `scrape-it` the ajax url (e.g. `example.com/api/that-endpoint`) and you will you will be able to parse the response",
          "**The ajax request is so complicated that you don't want to reverse-engineer it.** In this case, use a headless browser (e.g. Google Chrome, Electron, PhantomJS) to load the content and then use the `.scrapeHTML` method from scrape it once you get the HTML loaded on the page."
        ]
      },
      {
        "h3": "2. Crawling"
      },
      {
        "p": "There is no fancy way to crawl pages with `scrape-it`. For simple scenarios, you can parse the list of urls from the initial page and then, using Promises, parse each page. Also, you can use a different crawler to download the website and then use the `.scrapeHTML` method to scrape the local files."
      },
      {
        "h3": "3. Local files"
      },
      {
        "p": "Use the `.scrapeHTML` to parse the HTML read from the local files using `fs.readFile`."
      }
    ]
  },
  "bugs": {
    "url": "https://github.com/IonicaBizau/scrape-it/issues"
  },
  "contributors": [
    {
      "name": "ComFreek",
      "email": "comfreek@outlook.com",
      "url": "https://github.com/ComFreek"
    },
    {
      "name": "Jim Buck",
      "email": "jim@jimmyboh.com",
      "url": "https://github.com/JimmyBoh"
    }
  ],
  "dependencies": {
    "assured": "^1.0.12",
    "cheerio": "^0.22.0",
    "cheerio-req": "^1.2.2",
    "err": "^2.1.10",
    "is-empty-obj": "^1.0.10",
    "iterate-object": "^1.3.2",
    "obj-def": "^1.0.6",
    "typpy": "^2.3.10"
  },
  "description": "A Node.js scraper for humans.",
  "devDependencies": {
    "@types/cheerio": "^0.22.10",
    "lien": "^3.3.0",
    "tester": "^1.4.1"
  },
  "files": [
    "bin/",
    "app/",
    "lib/",
    "dist/",
    "src/",
    "scripts/",
    "resources/",
    "menu/",
    "cli.js",
    "index.js",
    "bloggify.js",
    "bloggify.json",
    "bloggify/"
  ],
  "homepage": "https://github.com/IonicaBizau/scrape-it#readme",
  "keywords": [
    "scrape",
    "it",
    "a",
    "scraping",
    "module",
    "for",
    "humans"
  ],
  "license": "MIT",
  "main": "lib/index.js",
  "name": "scrape-it",
  "repository": {
    "type": "git",
    "url": "git+ssh://git@github.com/IonicaBizau/scrape-it.git"
  },
  "scripts": {
    "test": "node test"
  },
  "types": "lib/index.d.ts",
  "version": "5.1.4"
}
